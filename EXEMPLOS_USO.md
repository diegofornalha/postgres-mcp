# Exemplos de Uso - PostgreSQL MCP

Este documento demonstra como usar as novas ferramentas de performance e monitoramento do PostgreSQL MCP.

## ğŸš€ Ferramentas de Performance

### 1. explain-query - AnÃ¡lise de Planos de ExecuÃ§Ã£o

#### Exemplo BÃ¡sico
```python
# Analisar plano de uma query simples
explain-query(query="SELECT * FROM users WHERE email = 'user@example.com'")
```

**SaÃ­da esperada:**
```
Query Execution Plan:
==================================================

Seq Scan on users  (cost=0.00..155.00 rows=1 width=200)
  Filter: (email = 'user@example.com'::text)

==================================================
Performance Analysis:

ğŸ“Œ Sequential scan on 'users'. May need an index if table is large.

--------------------------------------------------
ğŸ’¡ Tips:
â€¢ Run with analyze=True to see actual execution times
â€¢ Run with buffers=True to see buffer usage statistics
â€¢ Consider using EXPLAIN (VERBOSE) for more details
```

#### Exemplo com ANALYZE
```python
# Ver tempos reais de execuÃ§Ã£o
explain-query(
    query="SELECT u.*, COUNT(m.id) as message_count 
           FROM users u 
           LEFT JOIN messages m ON m.user_id = u.id 
           GROUP BY u.id",
    analyze=True
)
```

**SaÃ­da esperada:**
```
Query Execution Plan:
==================================================

HashAggregate  (cost=450.00..460.00 rows=100 width=208) (actual time=15.234..15.567 rows=100 loops=1)
  Group Key: u.id
  ->  Hash Right Join  (cost=25.00..400.00 rows=10000 width=204) (actual time=0.456..12.345 rows=10000 loops=1)
        Hash Cond: (m.user_id = u.id)
        ->  Seq Scan on messages m  (cost=0.00..350.00 rows=10000 width=4) (actual time=0.123..8.901 rows=10000 loops=1)
        ->  Hash  (cost=20.00..20.00 rows=100 width=200) (actual time=0.234..0.234 rows=100 loops=1)
              Buckets: 128  Batches: 1  Memory Usage: 24kB
              ->  Seq Scan on users u  (cost=0.00..20.00 rows=100 width=200) (actual time=0.012..0.189 rows=100 loops=1)

Planning Time: 0.234 ms
Execution Time: 15.789 ms

==================================================
Performance Analysis:

ğŸ“Š Timing Summary:
   - Planning Time: 0.23 ms
   - Execution Time: 15.79 ms
   - Total Time: 16.02 ms

âœ… Using hash join - good for large datasets.
ğŸ“Œ Sequential scan on 'messages'. May need an index if table is large.
ğŸ“Œ Sequential scan on 'users'. May need an index if table is large.

--------------------------------------------------
ğŸ’¡ Tips:
â€¢ Run with buffers=True to see buffer usage statistics
â€¢ Consider using EXPLAIN (VERBOSE) for more details
```

#### Exemplo com BUFFERS
```python
# AnÃ¡lise completa com uso de buffers
explain-query(
    query="SELECT * FROM large_table WHERE status = 'active' ORDER BY created_at DESC LIMIT 100",
    analyze=True,
    buffers=True
)
```

### 2. get-slow-queries - Identificar Queries Lentas

#### Exemplo BÃ¡sico
```python
# Buscar queries com tempo mÃ©dio > 1 segundo
get-slow-queries()
```

**SaÃ­da esperada:**
```
Slow Queries Report (threshold: 1000ms)
================================================================================

ğŸ”´ Query #1
Query: SELECT COUNT(*) FROM messages WHERE content LIKE '%search_term%'
Performance Stats:
  â€¢ Calls: 5,234
  â€¢ Mean Time: 2345.67 ms
  â€¢ Total Time: 12276783.78 ms (12276.78 seconds)
  â€¢ Min/Max: 1234.56 ms / 4567.89 ms
  â€¢ Std Dev: 890.12 ms
  â€¢ Rows/Call: 1.0
  â€¢ Cache Hit: 65.3%

ğŸ“Š Analysis:
â€¢ Leading wildcard in LIKE - cannot use index effectively
â€¢ Low cache hit ratio (65.3%) - consider increasing shared_buffers
â€¢ Very frequent execution - consider caching or query optimization

--------------------------------------------------------------------------------

ğŸ”´ Query #2
Query: SELECT DISTINCT user_id FROM activity_logs WHERE created_at > $1
Performance Stats:
  â€¢ Calls: 1,234
  â€¢ Mean Time: 1567.89 ms
  â€¢ Total Time: 1934693.46 ms (1934.69 seconds)
  â€¢ Min/Max: 987.65 ms / 2345.67 ms
  â€¢ Std Dev: 456.78 ms
  â€¢ Rows/Call: 234.5
  â€¢ Cache Hit: 89.2%

ğŸ“Š Analysis:
â€¢ DISTINCT can be expensive - ensure proper indexes
â€¢ Returning many rows per call - consider pagination or more selective filters

--------------------------------------------------------------------------------

ğŸ“ˆ Summary:
â€¢ Found 2 slow queries
â€¢ Threshold: 1000ms mean execution time
â€¢ Tip: Use explain-query tool to analyze specific queries in detail
```

#### Exemplo com ParÃ¢metros Customizados
```python
# Buscar queries mais rÃ¡pidas (> 500ms) e limitar a 5 resultados
get-slow-queries(min_duration_ms=500, limit=5)
```

### 3. health-check - VerificaÃ§Ã£o Completa de SaÃºde

#### Exemplo de Uso
```python
# Executar verificaÃ§Ã£o completa
health-check()
```

**SaÃ­da esperada:**
```
PostgreSQL Health Check Report
================================================================================

ğŸ”Œ Connection Statistics:
  â€¢ Active connections: 15
  â€¢ Idle connections: 25
  â€¢ Idle in transaction: 2
  â€¢ Waiting connections: 0
  â€¢ Total connections: 42/100 (42.0% used)

ğŸ’¾ Database Size:
  â€¢ Current size: 2.3 GB
  â€¢ Top 5 largest tables:
    - public.messages: 1.2 GB
    - public.activity_logs: 456 MB
    - public.users: 123 MB
    - public.files: 89 MB
    - public.sessions: 45 MB

ğŸ¯ Cache Performance:
  â€¢ Cache hit ratio: 98.45%
  âœ… Good cache performance

ğŸ§¹ Vacuum/Maintenance Status:
  â€¢ Tables with high dead tuple count:
    - public.messages: 5,234 dead tuples (2.1% of live)
    - public.activity_logs: 1,234 dead tuples (0.8% of live)

ğŸ”„ Replication Status:
  â€¢ Active replicas: 2
  âœ… All replicas in sync

â±ï¸  Long Running Queries:
  â€¢ PID 12345: Running for 00:08:23
    Query: SELECT * FROM generate_report($1, $2)...
  âš ï¸  Consider investigating these queries

ğŸˆ Potential Table Bloat:
  â€¢ public.messages: 1.2 GB (45% of total relation size)
  â€¢ public.users: 123 MB (38% of total relation size)
  ğŸ’¡ Low ratios may indicate index bloat

ğŸ“Š Overall Health Summary:
  â€¢ Health Score: 85/100 âš ï¸  GOOD (needs attention)
  â€¢ Issues found:
    - Long running queries detected

ğŸ’¡ Next Steps:
â€¢ Use get-slow-queries to identify performance bottlenecks
â€¢ Use explain-query to analyze problematic queries
â€¢ Monitor this health check regularly
```

## ğŸ¯ CenÃ¡rios de Uso Comum

### CenÃ¡rio 1: OtimizaÃ§Ã£o de Query Lenta

1. **Identificar queries lentas:**
```python
get-slow-queries(min_duration_ms=500)
```

2. **Analisar plano de execuÃ§Ã£o:**
```python
explain-query(
    query="[copiar query lenta aqui]",
    analyze=True,
    buffers=True
)
```

3. **Aplicar otimizaÃ§Ãµes sugeridas**

### CenÃ¡rio 2: Monitoramento de Rotina

1. **Verificar saÃºde geral:**
```python
health-check()
```

2. **Se score < 90, investigar queries:**
```python
get-slow-queries()
```

3. **Analisar queries problemÃ¡ticas especÃ­ficas**

### CenÃ¡rio 3: DiagnÃ³stico de Performance

1. **Query especÃ­fica estÃ¡ lenta?**
```python
explain-query(query="SELECT ...", analyze=True)
```

2. **Verificar se Ã© um padrÃ£o:**
```python
get-slow-queries(min_duration_ms=100, limit=50)
```

3. **Verificar saÃºde geral do banco:**
```python
health-check()
```

## ğŸ’¡ Dicas Importantes

1. **Use `analyze=True` com cuidado** - ele executa a query real
2. **Para queries UPDATE/DELETE** - remova a parte de modificaÃ§Ã£o e analise apenas o SELECT
3. **Cache hit ratio** - deve estar acima de 90% para boa performance
4. **Dead tuples** - acima de 20% indica necessidade de VACUUM
5. **pg_stat_statements** - precisa estar habilitado para get-slow-queries funcionar

## ğŸ”§ Troubleshooting

### "pg_stat_statements not found"
```sql
-- Como DBA, execute:
CREATE EXTENSION pg_stat_statements;
```

### "Permission denied"
- Certifique-se que o usuÃ¡rio tem permissÃ£o SELECT nas tabelas pg_stat_*

### Query muito complexa para EXPLAIN
- Divida em partes menores
- Use CTEs (WITH clauses) para simplificar

## ğŸ“Š Interpretando Resultados

### Planos de ExecuÃ§Ã£o
- **Seq Scan**: Leitura sequencial - ruim para tabelas grandes
- **Index Scan**: Uso de Ã­ndice - bom para queries seletivas
- **Bitmap Scan**: MÃºltiplas linhas via Ã­ndice - eficiente
- **Hash Join**: Bom para grandes conjuntos
- **Nested Loop**: Pode ser lento com muitas linhas

### MÃ©tricas de Performance
- **Mean time > 1000ms**: Query definitivamente precisa otimizaÃ§Ã£o
- **Cache hit < 90%**: Aumentar shared_buffers
- **High std dev**: Performance inconsistente
- **Many calls**: Candidata para caching

### Health Score
- **90-100**: Excelente
- **70-89**: Bom, mas precisa atenÃ§Ã£o
- **< 70**: AÃ§Ã£o imediata necessÃ¡ria